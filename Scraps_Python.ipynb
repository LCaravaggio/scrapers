{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu/miXtjCfI0usatli/zSI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/scrapers/blob/master/Scraps_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapers"
      ],
      "metadata": {
        "id": "N5YzbMEIr0D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "from urllib.request import urlopen, Request\n",
        "import datetime "
      ],
      "metadata": {
        "id": "pG5D7AZQpmo9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4w6IkoeEpfyr"
      },
      "outputs": [],
      "source": [
        "def scrapvea(sitevea):\n",
        "    try:\n",
        "      r = requests.get(sitevea, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "      b=\"\" \n",
        "      soup = BeautifulSoup(r.content, 'html.parser')\n",
        "      b+=soup.find('span', {'class':'vtex-store-components-3-x-productBrand'}).text.replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\") + \";\"\n",
        "      ini=str(soup).find('\"teasers\":[],\"Price\":')\n",
        "      fin=str(soup).find(',\"ListPrice\":')\n",
        "      b+=str(soup)[ini+21:fin]\n",
        "      \n",
        "      return b\n",
        "    except Exception as e:\n",
        "      return \"no se pudo acceder a VEA\"\n",
        "\n",
        "\n",
        "def scrapcoto(sitecoto):\n",
        "    try:\n",
        "      r = requests.get(sitecoto, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "      b=\"\"\n",
        "      soup = BeautifulSoup(r.content, 'html.parser')\n",
        "      b+=soup.find(\"h1\", {\"class\": \"product_page\"}).text.replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\";\",\"\").replace(\"\\t\",\"\") + \";\"     \n",
        "      b+=soup.find(\"span\", {\"class\": \"atg_store_newPrice\"}).text.replace(\"$\",\"\").replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\").replace(\"PRECIOCONTADO\",\"\").replace(\"PRECIOREGULAR\",\"\") \n",
        "      return b\n",
        "    except Exception as e:\n",
        "      return \"no se pudo acceder a COTO\"\n",
        "\n",
        "def scrapcarrefour(sitecarrefour):\n",
        "    try: \n",
        "      r = requests.get(sitecarrefour, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "      b=\"\"\n",
        "      soup = BeautifulSoup(r.content, 'html.parser')\n",
        "      ini=str(soup).find('=\"og:type\"/><meta content=\"')\n",
        "      fin=str(soup).find('\" data-react-helmet=\"true\" property=\"og:title\"/><meta ')\n",
        "      b+=str(soup)[ini+27:fin]\n",
        "      b+=\";\"\n",
        "      ini=str(soup).find('\"typename\":\"Teaser\"}],\"Price\":')\n",
        "      fin=str(soup).find(',\"ListPrice\":')\n",
        "      b+=str(soup)[ini+30:fin]\n",
        "      \n",
        "      return b\n",
        "    except Exception as e:\n",
        "      return \"no se pudo acceder a CARREFOUR\"\n",
        "\n",
        "def scrapdia(sitedia):\n",
        "    try:\n",
        "      r = requests.get(sitedia, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "      b=\"\"\n",
        "      soup = BeautifulSoup(r.content, 'html.parser')\n",
        "      ini=str(soup).find(',\"name\":\"')\n",
        "      fin=str(soup).find('\",\"brand\":{\"@type\":')\n",
        "      if (fin-ini)<120: b+=str(soup)[ini+9:fin].replace(\";\",\"\")\n",
        "      b+=\";\"\n",
        "      ini=str(soup).find('\"product:availability\"/><meta content=\"')\n",
        "      fin=str(soup).find('\" data-react-helmet=\"true\" property=\"product:price:amount\"/>')\n",
        "      if (fin!=-1) & ((fin-ini-39)<10): b+=str(soup)[ini+39:fin]\n",
        "      \n",
        "      if len(b)<100: \n",
        "        return b\n",
        "      else: \n",
        "        return \"no se pudo acceder a DIA\"\n",
        "\n",
        "    except Exception as e:\n",
        "      return \"no se pudo acceder a DIA\"\n",
        "\n",
        "def scrapadidas(siteadidas): \n",
        "  try: \n",
        "    r = requests.get(siteadidas, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find('id=\"meta-title\">')\n",
        "    fin=str(soup).find('</title>')\n",
        "    b+=str(soup)[ini+16:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"ARS\",\"price\"')\n",
        "    fin=str(soup).find(',\"availability\"')\n",
        "    if fin!=-1: b+=str(soup)[ini+14:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a ADIDAS\"\n",
        "\n",
        "def scrapdexter(sitedexter): \n",
        "  try: \n",
        "    r = requests.get(sitedexter, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"h1\", {\"class\": \"product-name\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": 'sales'}).text.replace('\\n', '').replace(' ', '')\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a DEXTER\"\n",
        "\n",
        "\n",
        "def scrapbensimon (sitebensimon): \n",
        "  try: \n",
        "    r = requests.get(sitebensimon, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find('\",\"productName\":')\n",
        "    fin=str(soup).find('\",\"productReference\":\"')\n",
        "    b+=str(soup)[ini+17:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('{\"highPrice\":')\n",
        "    fin=str(soup).find(',\"lowPrice\":')\n",
        "    b+=str(soup)[ini+13:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a BENSIMON\"\n",
        "\n",
        "def scrapmacowens (sitemacowens): \n",
        "  try: \n",
        "    r = requests.get(sitemacowens, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"base\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": \"price\"}).text\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a MACOWENS\"\n",
        "\n",
        "def scrapkevingston (sitekevingston): \n",
        "  try: \n",
        "    r = requests.get(sitekevingston, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"input\", {\"id\": \"nomproducto\"})['value']\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"input\", {\"id\": \"priceprod\"})['value']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a KEVINGSTON\"\n",
        "\n",
        "def scrapequus (siteequus): \n",
        "  try: \n",
        "    r = requests.get(siteequus, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find(',\"description\":\"')\n",
        "    fin=str(soup).find('\",\"mpn\":\"')\n",
        "    if ini!=-1: b+=str(soup)[ini+16:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"lowPrice\":')\n",
        "    fin=str(soup).find(',\"highPrice\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+11:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a EQUUS\"\n",
        "\n",
        "def scrapcuestablanca (sitecuestablanca): \n",
        "  try: \n",
        "    r = requests.get(sitecuestablanca, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find(',\"name\":\"')\n",
        "    fin=str(soup).find('\",\"salesChannel\":\"')\n",
        "    if fin!=-1: b+=str(soup)[ini+9:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\",\"productPriceTo\":\"')\n",
        "    fin=str(soup).find('\",\"sellerId\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+20:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a CUESTA BLANCA\"\n",
        "\n",
        "def scraplaspepas (sitelaspepas): \n",
        "  try: \n",
        "    r = requests.get(sitelaspepas, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"value\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": \"price-wrapper \"})['data-price-amount']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a LAS PEPAS\"\n",
        "\n",
        "def scrapportsaid (siteportsaid): \n",
        "  try: \n",
        "    r = requests.get(siteportsaid, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find(',\"name\":\"')\n",
        "    fin=str(soup).find('\",\"salesChannel\":\"')\n",
        "    b+=str(soup)[ini+9:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\",\"productPriceTo\":\"')\n",
        "    fin=str(soup).find('\",\"sellerId\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+20:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a PORTSAID\"\n",
        "\n",
        "def scrapmimo (sitemimo):\n",
        "  try: \n",
        "    r = requests.get(sitemimo, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"vtex-breadcrumb-1-x-term ph2 c-on-base\"}).text\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"lowPrice\":')\n",
        "    fin=str(soup).find(',\"highPrice\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+11:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a MIMO\"\n",
        "\n",
        "def scrapcheeky (sitecheeky): \n",
        "  try: \n",
        "    r = requests.get(sitecheeky, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find('<title class=\"js-title\">\\n')\n",
        "    fin=str(soup).find(' | Cheeky\\n</title>')\n",
        "    b+=str(soup)[ini+25:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find(\"'price': '\")\n",
        "    fin=str(soup).find(\"'item_brand': \")\n",
        "    if fin!=-1: b+=str(soup)[ini+10:fin-5]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a CHEEKY\"\n",
        "\n",
        "def scrapselu (siteselu): \n",
        "  try: \n",
        "    r = requests.get(siteselu, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"div\", {\"class\": \"product-name\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"input\", {\"id\": \"___rc-p-dv-id\"})['value']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a SELÚ\"\n",
        "\n",
        "def scrapcarocuore (sitecarocuore):\n",
        "  try: \n",
        "    r = requests.get(sitecarocuore, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"base\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": \"price-wrapper\"}).text\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a CARO CUORE\"\n",
        "\n",
        "def scrapcocot (sitecocot): \n",
        "  try: \n",
        "    r = requests.get(sitecocot, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"base\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": \"price-wrapper\"})['data-price-amount']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a COCOT\"\n",
        "\n",
        "def scrapsweetvictorian (sitesweetvictorian): \n",
        "  try: \n",
        "    r = requests.get(sitesweetvictorian, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"p\", {\"class\": \"font-weight-bolder\"}).text.replace('\\n', '')\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"ProductPrice\":')\n",
        "    fin=str(soup).find(',\"AddToCart\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+15:fin-2]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a SWEET VICTORIAN\"\n",
        "\n",
        "def scraptropea (sitetropea): \n",
        "  try: \n",
        "    r = requests.get(sitetropea, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"base\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": \"price-wrapper\"})['data-price-amount']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a TROPEA\"\n",
        "\n",
        "def scrapxlshop (sitexlshop): \n",
        "  try: \n",
        "    r = requests.get(sitexlshop, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"div\", {\"class\": \"name\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"input\", {\"id\": \"___rc-p-dv-id\"})['value']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a XL SHOP\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def corre(lista, nombre): \n",
        "  a=\"\"\n",
        "  for l in lista:\n",
        "    a+= l + \";\" \n",
        "    if l[:19] == 'https://www.vea.com': a+=scrapvea(l)\n",
        "    if l[:19] == 'https://www.cotodig': a+=scrapcoto(l)\n",
        "    if l[:19] == 'https://www.carrefo': a+=scrapcarrefour(l)\n",
        "    if l[:19] == 'https://diaonline.s': a+=scrapdia(l)\n",
        "    if l[:19] == 'https://www.adidas.': a+=scrapadidas(l)\n",
        "    if l[:19] == 'https://www.dexter.': a+=scrapdexter(l)\n",
        "    if l[:19] == 'https://www.bensimo': a+=scrapbensimon(l)\n",
        "    if l[:19] == 'https://www.macowen': a+=scrapmacowens(l)\n",
        "    if l[:19] == 'https://www.kevings': a+=scrapkevingston(l)\n",
        "    if l[:19] == 'https://www.equus.c': a+=scrapequus(l)\n",
        "    if l[:19] == 'https://www.cuestab': a+=scrapcuestablanca(l)\n",
        "    if l[:19] == 'https://www.laspepa': a+=scraplaspepas(l)\n",
        "    if l[:19] == 'https://www.portsai': a+=scrapportsaid(l)\n",
        "    if l[:19] == 'https://www.mimo.co': a+=scrapmimo(l)\n",
        "    if l[:19] == 'https://www.cheeky.': a+=scrapcheeky(l)\n",
        "    if l[:19] == 'https://www.selu.co': a+=scrapselu(l)\n",
        "    if l[:19] == 'https://www.carocuo': a+=scrapcarocuore(l)\n",
        "    if l[:19] == 'https://cocotonline': a+=scrapcocot(l)\n",
        "    if l[:19] == 'https://www.sweetvi': a+=scrapsweetvictorian(l)\n",
        "    if l[:19] == 'https://tropea.com.': a+=scraptropea(l)\n",
        "    if l[:19] == 'https://www.xlshop.': a+=scrapxlshop(l)\n",
        "    a+=\"\\n\"\n",
        "      \n",
        "  now = datetime.datetime.now()\n",
        "  nw=str(now.strftime(\"%Y-%m-%d %H-%M-%S\"))\n",
        "  \n",
        "  with open('/content/' + str(nombre) + nw + '.csv', 'w', newline=\"\\n\", encoding='ISO-8859-1') as f:\n",
        "    f.write(a)\n",
        "    f.close"
      ],
      "metadata": {
        "id": "_jeJNkJ_rpAD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/indumentaria.txt') as file:\n",
        "    lista = [line.rstrip() for line in file]\n",
        "    corre(lista, 'indumentaria')"
      ],
      "metadata": {
        "id": "6G_bjY3GJ7In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/supermercados.txt') as file:\n",
        "    lista = [line.rstrip() for line in file]\n",
        "    corre(lista, 'supermercados')"
      ],
      "metadata": {
        "id": "2UjYGrvUqo1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}