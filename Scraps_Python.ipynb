{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaKfdFINxeKJLzf12aM5Uj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/scrapers/blob/master/Scraps_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "from urllib.request import urlopen, Request\n",
        "import datetime "
      ],
      "metadata": {
        "id": "pG5D7AZQpmo9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "4w6IkoeEpfyr"
      },
      "outputs": [],
      "source": [
        "def scrapvea(sitevea):\n",
        "    try:\n",
        "      r = requests.get(sitevea)\n",
        "      b=\"\" \n",
        "      soup = BeautifulSoup(r.content, 'html.parser')\n",
        "      b+=soup.find('span', {'class':'vtex-store-components-3-x-productBrand'}).text.replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\") + \";\"\n",
        "      ini=str(soup).find('\"teasers\":[],\"Price\":')\n",
        "      fin=str(soup).find(',\"ListPrice\":')\n",
        "      b+=str(soup)[ini+21:fin]\n",
        "      return b\n",
        "    except Exception as e:\n",
        "      return \"no se pudo acceder a VEA\"\n",
        "\n",
        "\n",
        "def scrapcoto(sitecoto):\n",
        "    try:\n",
        "      r = requests.get(sitecoto)\n",
        "      b=\"\"\n",
        "      soup = BeautifulSoup(r.content, 'html.parser')\n",
        "      b+=soup.find(\"h1\", {\"class\": \"product_page\"}).text.replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\";\",\"\").replace(\"\\t\",\"\") + \";\"     \n",
        "      b+=soup.find(\"span\", {\"class\": \"atg_store_newPrice\"}).text.replace(\"$\",\"\").replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\").replace(\"PRECIOCONTADO\",\"\").replace(\"PRECIOREGULAR\",\"\") \n",
        "      return b\n",
        "    except Exception as e:\n",
        "      return \"no se pudo acceder a COTO\"\n",
        "\n",
        "def scrapcarrefour(sitecarrefour):\n",
        "    try: \n",
        "      r = requests.get(sitecarrefour)\n",
        "      b=\"\"\n",
        "      soup = BeautifulSoup(r.content, 'html.parser')\n",
        "      ini=str(soup).find('=\"og:type\"/><meta content=\"')\n",
        "      fin=str(soup).find('\" data-react-helmet=\"true\" property=\"og:title\"/><meta ')\n",
        "      b+=str(soup)[ini+27:fin]\n",
        "      b+=\";\"\n",
        "      ini=str(soup).find('\"typename\":\"Teaser\"}],\"Price\":')\n",
        "      fin=str(soup).find(',\"ListPrice\":')\n",
        "      b+=str(soup)[ini+30:fin]\n",
        "      return b\n",
        "    except Exception as e:\n",
        "      return \"no se pudo acceder a CARREFOUR\"\n",
        "\n",
        "def scrapdia(sitedia):\n",
        "    try:\n",
        "      r = requests.get(sitedia)\n",
        "      b=\"\"\n",
        "      soup = BeautifulSoup(r.content, 'html.parser')\n",
        "      ini=str(soup).find(',\"name\":\"')\n",
        "      fin=str(soup).find('\",\"brand\":{\"@type\":')\n",
        "      b+=str(soup)[ini+9:fin]\n",
        "      b+=\";\"\n",
        "      ini=str(soup).find('\"product:availability\"/><meta content=\"')\n",
        "      fin=str(soup).find('\" data-react-helmet=\"true\" property=\"product:price:amount\"/>')\n",
        "      if fin!=-1: b+=str(soup)[ini+39:fin]\n",
        "    \n",
        "      return b\n",
        "    except Exception as e:\n",
        "      return \"no se pudo acceder a DIA\"\n",
        "\n",
        "def scrapadidas(siteadidas): \n",
        "  try: \n",
        "    r = requests.get(siteadidas, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find('id=\"meta-title\">')\n",
        "    fin=str(soup).find('</title>')\n",
        "    b+=str(soup)[ini+16:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"ARS\",\"price\"')\n",
        "    fin=str(soup).find(',\"availability\"')\n",
        "    if fin!=-1: b+=str(soup)[ini+14:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a ADIDAS\"\n",
        "\n",
        "def scrapdexter(sitedexter): \n",
        "  try: \n",
        "    r = requests.get(sitedexter, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find('\"Product\",\"name\":\"')\n",
        "    fin=str(soup).find('\",\"description\"')\n",
        "    b+=str(soup)[ini+18:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"ARS\",\"price\":\"')\n",
        "    fin=str(soup).find(',\"availability\"')\n",
        "    if fin!=-1: b+=str(soup)[ini+16:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a DEXTER\"\n",
        "\n",
        "\n",
        "def scrapbensimon (sitebensimon): \n",
        "  try: \n",
        "    r = requests.get(sitebensimon, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find('data-react-helmet=\"true\" property=\"og:url\"/><meta content=\"')\n",
        "    fin=str(soup).find('\" data-react-helmet=\"true\" property=\"og:description\"')\n",
        "    b+=str(soup)[ini+59:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"lowPrice\":')\n",
        "    fin=str(soup).find(',\"highPrice\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+11:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a BENSIMON\"\n",
        "\n",
        "def scrapmacowens (sitemacowens): \n",
        "  try: \n",
        "    r = requests.get(sitemacowens, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find('{\"products\":[{\"name\":\"')\n",
        "    fin=str(soup).find('\",\"id\":\"')\n",
        "    b+=str(soup)[ini+22:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"price\":\"')\n",
        "    fin=str(soup).find('\",\"brand\":\"')\n",
        "    if fin!=-1: b+=str(soup)[ini+9:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a MACOWENS\"\n",
        "\n",
        "def scrapkevingston (sitekevingston): \n",
        "  try: \n",
        "    r = requests.get(sitekevingston, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"input\", {\"id\": \"nomproducto\"})['value']\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"input\", {\"id\": \"priceprod\"})['value']\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a KEVINGSTON\"\n",
        "\n",
        "def scrapequus (siteequus): \n",
        "  try: \n",
        "    r = requests.get(siteequus, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find(',\"description\":\"')\n",
        "    fin=str(soup).find('\",\"mpn\":\"')\n",
        "    b+=str(soup)[ini+16:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"lowPrice\":')\n",
        "    fin=str(soup).find(',\"highPrice\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+11:fin]\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a EQUUS\"\n",
        "\n",
        "def scrapcuestablanca (sitecuestablanca): \n",
        "  try: \n",
        "    r = requests.get(sitecuestablanca, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find(',\"name\":\"')\n",
        "    fin=str(soup).find('\",\"salesChannel\":\"')\n",
        "    b+=str(soup)[ini+9:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\",\"productPriceTo\":\"')\n",
        "    fin=str(soup).find('\",\"sellerId\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+20:fin]\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a CUESTA BLANCA\"\n",
        "\n",
        "def scraplaspepas (sitelaspepas): \n",
        "  try: \n",
        "    r = requests.get(sitelaspepas, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"value\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": \"price-wrapper \"})['data-price-amount']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a LAS PEPAS\"\n",
        "\n",
        "def scrapportsaid (siteportsaid): \n",
        "  try: \n",
        "    r = requests.get(siteportsaid, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find(',\"name\":\"')\n",
        "    fin=str(soup).find('\",\"salesChannel\":\"')\n",
        "    b+=str(soup)[ini+9:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\",\"productPriceTo\":\"')\n",
        "    fin=str(soup).find('\",\"sellerId\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+20:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a PORTSAID\"\n",
        "\n",
        "def scrapmimo (sitemimo):\n",
        "  try: \n",
        "    r = requests.get(sitemimo, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find(',\"name\":\"')\n",
        "    fin=str(soup).find('\",\"brand\":{\"@type\":\"')\n",
        "    b+=str(soup)[ini+9:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"lowPrice\":')\n",
        "    fin=str(soup).find(',\"highPrice\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+11:fin]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a MIMO\"\n",
        "\n",
        "def scrapcheeky (sitecheeky): \n",
        "  try: \n",
        "    r = requests.get(sitecheeky, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    ini=str(soup).find('<title class=\"js-title\">\\n')\n",
        "    fin=str(soup).find(' | Cheeky\\n</title>')\n",
        "    b+=str(soup)[ini+25:fin]\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find(\"'price': '\")\n",
        "    fin=str(soup).find(\"'item_brand': \")\n",
        "    if fin!=-1: b+=str(soup)[ini+10:fin-5]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a CHEEKY\"\n",
        "\n",
        "def scrapselu (siteselu): \n",
        "  try: \n",
        "    r = requests.get(siteselu, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"div\", {\"class\": \"product-name\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"input\", {\"id\": \"___rc-p-dv-id\"})['value']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a SELÚ\"\n",
        "\n",
        "def scrapcarocuore (sitecarocuore):\n",
        "  try: \n",
        "    r = requests.get(sitecarocuore, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"base\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": \"price-wrapper \"}).text\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a CARO CUORE\"\n",
        "\n",
        "def scrapcocot (sitecocot): \n",
        "  try: \n",
        "    r = requests.get(sitecocot, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"base\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": \"price-wrapper \"})['data-price-amount']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a COCOT\"\n",
        "\n",
        "def scrapsweetvictorian (sitesweetvictorian): \n",
        "  try: \n",
        "    r = requests.get(sitesweetvictorian, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"p\", {\"class\": \"font-weight-bolder\"}).text.replace('\\n', '')\n",
        "    b+=\";\"\n",
        "    ini=str(soup).find('\"ProductPrice\":')\n",
        "    fin=str(soup).find(',\"AddToCart\":')\n",
        "    if fin!=-1: b+=str(soup)[ini+15:fin-2]\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a SWEET VICTORIAN\"\n",
        "\n",
        "def scraptropea (sitetropea): \n",
        "  try: \n",
        "    r = requests.get(sitetropea, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"span\", {\"class\": \"base\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"span\", {\"class\": \"price-wrapper \"})['data-price-amount']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a TROPEA\"\n",
        "\n",
        "def scrapxlshop (sitexlshop): \n",
        "  try: \n",
        "    r = requests.get('https://www.xlshop.com.ar/carteras-nuria-tote-xv2sdc33c0813/p', headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
        "    b=\"\"\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    b+=soup.find(\"div\", {\"class\": \"name\"}).text\n",
        "    b+=\";\"\n",
        "    b+=soup.find(\"input\", {\"id\": \"___rc-p-dv-id\"})['value']\n",
        "    return b\n",
        "  except Exception as e:\n",
        "    return \"no se pudo acceder a XL SHOP\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lista=['https://www.adidas.com.ar/zapatillas-zntasy-lightmotion-/GZ2313.html', \n",
        "       'https://www.sweetvictorian.com.ar/producto/1115/37/wake-up-pijama']"
      ],
      "metadata": {
        "id": "OFsIOHg2pzMw"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=\"\"\n",
        "for l in lista:\n",
        "  a = a + l + \";\" \n",
        "  if l[:19] == 'https://www.vea.com': a=a+scrapvea(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.cotodig': a=a+scrapcoto(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.carrefo': a=a+scrapcarrefour(l)+\"\\n\"\n",
        "  if l[:19] == 'https://diaonline.s': a=a+scrapdia(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.adidas.': a=a+scrapadidas(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.dexter.': a=a+scrapdexter(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.bensimo': a=a+scrapbensimon(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.macowen': a=a+scrapmacowens(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.kevings': a=a+scrapkevingston(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.equus.c': a=a+scrapequus(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.cuestab': a=a+scrapcuestablanca(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.laspepa': a=a+scraplaspepas(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.portsai': a=a+scrapportsaid(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.mimo.co': a=a+scrapmimo(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.cheeky.': a=a+scrapcheeky(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.selu.co': a=a+scrapselu(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.carocuo': a=a+scrapcarocuore(l)+\"\\n\"\n",
        "  if l[:19] == 'https://cocotonline': a=a+scrapcocot(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.sweetvi': a=a+scrapsweetvictorian(l)+\"\\n\"\n",
        "  if l[:19] == 'https://tropea.com.': a=a+scraptropea(l)+\"\\n\"\n",
        "  if l[:19] == 'https://www.xlshop.': a=a+scrapxlshop(l)+\"\\n\""
      ],
      "metadata": {
        "id": "_jeJNkJ_rpAD"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.datetime.now()\n",
        "nw=str(now.strftime(\"%Y-%m-%d %H-%M-%S\"))\n",
        "with open('/content/' + nw + '.csv', 'w', newline=\"\\n\", encoding='ISO-8859-1') as f:\n",
        "\tf.write(a)\n",
        "f.close"
      ],
      "metadata": {
        "id": "V5nv4YV1ZDS4",
        "outputId": "aea6b6ec-2b9d-4e0b-e762-8421a6be393d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function TextIOWrapper.close()>"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    }
  ]
}